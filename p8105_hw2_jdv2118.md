p8105_hw2_jdv2118
================

# Loading libraries

The code below is used to load the libraries that are needed to complete
the problems for homework 2.

``` r
library(tidyverse)

library(readxl)

library(dplyr)

library(janitor)

library(tidyr)

library(lubridate)
```

# Problem 1

Please note that the solution for Problem 1 of homework 2 for Data
Science I was provided by Dr. Goldsmith prior to the due date of
homework 2. This code is seen below to use for future reference.

Below we import and clean data from
`NYC_Transit_Subway_Entrance_And_Exit_Data.csv`. The process begins with
data import, updates variable names, and selects the columns that will
be used in later parts fo this problem. We update `entry` from `yes` /
`no` to a logical variable. As part of data import, we specify that
`Route` columns 8-11 should be character for consistency with 1-7.

``` r
trans_ent = 
  read_csv(
    "data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv",
    col_types = cols(Route8 = "c", Route9 = "c", Route10 = "c", Route11 = "c")) %>% 
  janitor::clean_names() %>% 
  select(
    line, station_name, station_latitude, station_longitude, 
    starts_with("route"), entry, exit_only, vending, entrance_type, 
    ada) %>% 
  mutate(entry = ifelse(entry == "YES", TRUE, FALSE))
```

As it stands, these data are not “tidy”: route number should be a
variable, as should route. That is, to obtain a tidy dataset we would
need to convert `route` variables from wide to long format. This will be
useful when focusing on specific routes, but may not be necessary when
considering questions that focus on station-level variables.

The following code chunk selects station name and line, and then uses
`distinct()` to obtain all unique combinations. As a result, the number
of rows in this dataset is the number of unique stations.

``` r
trans_ent %>% 
  select(station_name, line) %>% 
  distinct
```

    ## # A tibble: 465 × 2
    ##    station_name             line    
    ##    <chr>                    <chr>   
    ##  1 25th St                  4 Avenue
    ##  2 36th St                  4 Avenue
    ##  3 45th St                  4 Avenue
    ##  4 53rd St                  4 Avenue
    ##  5 59th St                  4 Avenue
    ##  6 77th St                  4 Avenue
    ##  7 86th St                  4 Avenue
    ##  8 95th St                  4 Avenue
    ##  9 9th St                   4 Avenue
    ## 10 Atlantic Av-Barclays Ctr 4 Avenue
    ## # … with 455 more rows

The next code chunk is similar, but filters according to ADA compliance
as an initial step. This produces a dataframe in which the number of
rows is the number of ADA compliant stations.

``` r
trans_ent %>% 
  filter(ada == TRUE) %>% 
  select(station_name, line) %>% 
  distinct
```

    ## # A tibble: 84 × 2
    ##    station_name                   line           
    ##    <chr>                          <chr>          
    ##  1 Atlantic Av-Barclays Ctr       4 Avenue       
    ##  2 DeKalb Av                      4 Avenue       
    ##  3 Pacific St                     4 Avenue       
    ##  4 Grand Central                  42nd St Shuttle
    ##  5 34th St                        6 Avenue       
    ##  6 47-50th Sts Rockefeller Center 6 Avenue       
    ##  7 Church Av                      6 Avenue       
    ##  8 21st St                        63rd Street    
    ##  9 Lexington Av                   63rd Street    
    ## 10 Roosevelt Island               63rd Street    
    ## # … with 74 more rows

To compute the proportion of station entrances / exits without vending
allow entrance, we first exclude station entrances that do not allow
vending. Then, we focus on the `entry` variable – this logical, so
taking the mean will produce the desired proportion (recall that R will
coerce logical to numeric in cases like this).

``` r
trans_ent %>% 
  filter(vending == "NO") %>% 
  pull(entry) %>% 
  mean
```

    ## [1] 0.3770492

Lastly, we write a code chunk to identify stations that serve the A
train, and to assess how many of these are ADA compliant. As a first
step, we tidy the data as alluded to previously; that is, we convert
`route` from wide to long format. After this step, we can use tools from
previous parts of the question (filtering to focus on the A train, and
on ADA compliance; selecting and using `distinct` to obtain dataframes
with the required stations in rows).

``` r
trans_ent %>% 
  pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") %>% 
  filter(route == "A") %>% 
  select(station_name, line) %>% 
  distinct
```

    ## # A tibble: 60 × 2
    ##    station_name                  line           
    ##    <chr>                         <chr>          
    ##  1 Times Square                  42nd St Shuttle
    ##  2 125th St                      8 Avenue       
    ##  3 145th St                      8 Avenue       
    ##  4 14th St                       8 Avenue       
    ##  5 168th St - Washington Heights 8 Avenue       
    ##  6 175th St                      8 Avenue       
    ##  7 181st St                      8 Avenue       
    ##  8 190th St                      8 Avenue       
    ##  9 34th St                       8 Avenue       
    ## 10 42nd St                       8 Avenue       
    ## # … with 50 more rows

``` r
trans_ent %>% 
  pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") %>% 
  filter(route == "A", ada == TRUE) %>% 
  select(station_name, line) %>% 
  distinct
```

    ## # A tibble: 17 × 2
    ##    station_name                  line            
    ##    <chr>                         <chr>           
    ##  1 14th St                       8 Avenue        
    ##  2 168th St - Washington Heights 8 Avenue        
    ##  3 175th St                      8 Avenue        
    ##  4 34th St                       8 Avenue        
    ##  5 42nd St                       8 Avenue        
    ##  6 59th St                       8 Avenue        
    ##  7 Inwood - 207th St             8 Avenue        
    ##  8 West 4th St                   8 Avenue        
    ##  9 World Trade Center            8 Avenue        
    ## 10 Times Square-42nd St          Broadway        
    ## 11 59th St-Columbus Circle       Broadway-7th Ave
    ## 12 Times Square                  Broadway-7th Ave
    ## 13 8th Av                        Canarsie        
    ## 14 Franklin Av                   Franklin        
    ## 15 Euclid Av                     Fulton          
    ## 16 Franklin Av                   Fulton          
    ## 17 Howard Beach                  Rockaway

# Problem 2

## Mr. Trash Wheel

The code below is used to read and clean the “Mr. Trash Wheel” sheet.

``` r
mr_trash_wheel = 
  read_excel("data/Trash Wheel Collection Data.xlsx", "Mr. Trash Wheel", range = "A2:N550") %>%
  clean_names(dat = .) %>%
  mutate(
  sports_balls = 
  as.integer(
  round(sports_balls)), 
  trash_wheel_id = 1, 
  year =
  as.integer(year)) %>%
  drop_na(data = ., dumpster) %>%
  select(trash_wheel_id, everything())

view(mr_trash_wheel)
```

## Professor Trash Wheel

The code below is used to import, clean, and organize the data for the
“Professor Trash Wheel” sheet.

``` r
prof_trash_wheel = 
  read_excel("data/Trash Wheel Collection Data.xlsx", "Professor Trash Wheel", range =           "A2:M97") %>%
  clean_names(dat = .) %>%
  mutate(
  trash_wheel_id = 2) %>%
  drop_na(data = ., dumpster) %>%
  select(trash_wheel_id, everything())

view(prof_trash_wheel)
```

## Combined dataset

``` r
trash_wheel_tidy =
  bind_rows(mr_trash_wheel, prof_trash_wheel)

view(trash_wheel_tidy)
```

The final dataset, trash_wheel_tidy, combines data from both the
Mr. Trash Wheel and Professor Trash Wheel datasets. The mr_trash_wheel
dataset consists of 547 rows and 15 columns. The prof_trash_wheel
dataset consists of 94 rows and 14 columns. The trash_wheel_tidy dataset
consists of 641 observations. This dataset also consists of 15
variables, which are  
trash_wheel_id, dumpster, month, year, date, weight_tons,
volume_cubic_yards, plastic_bottles, polystyrene, cigarette_butts,
glass_bottles, grocery_bags, chip_bags, sports_balls, and homes_powered.
It is important to note that the trash_wheel_id variable is imperative
as it indicates Mr. Trash Wheel for the value of 1 and indicates
Professor Trash Wheel for the value of 2. In addition, the Professor
Trash Wheel dataset consisted of all the same variables as the Mr. Trash
Wheel dataset except for the sports_balls variable. This dataset shows
that the total weight of trash that was collected by Professor Trash
Wheel is 190.12 tons. This dataset also shows that the total number of
sports balls that were collected by Mr. Trash Wheel in 2020 was 856.

# Problem 3

## Pols_month dataset

The code below is used to clean the data within the pols_month dataset.

``` r
pols_month =
  read_csv("data/pols-month.csv") %>%
  clean_names(dat = .) %>%
  separate(col = mon,   into = c("year", "month", "day"), sep = "-") %>%
  mutate(
    year = as.numeric(year), 
    month = as.numeric(month), 
    day = as.numeric(day),
    month = month.name[as.numeric(month)],
    president = 
      if_else(prez_gop == 1, 'gop', 'dem')) %>% 
    select(-prez_dem, -prez_gop, -day) %>%
  arrange(year, month)

view(pols_month)
```

## Snp dataset

The code below is used to clean the data within the snp dataset.

``` r
snp = 
  read_csv("data/snp.csv") %>%
  clean_names(dat = .) %>%
  mutate(date = parse_date_time2(date,'mdy', cutoff_2000 = 49)) %>%
  separate(col = date,   into = c("year", "month", "day"), sep = "-") %>%
  mutate( 
    year = as.numeric(year),
    month = month.name[as.numeric(month)]) %>% 
    select(-day) %>%
  arrange(year, month) %>%
  select(year, month, everything())

view(snp)
```

## Unemployment dataset

The code below is used to clean the data within the unemployment
dataset.

``` r
unemployment =
  read_csv("data/unemployment.csv") %>%
  clean_names(dat = .) %>%
  pivot_longer(
    jan:dec,
    names_to = "month", 
    values_to = "unemployment_percentage"
    ) %>%
  mutate(month = recode(month, 'jan' = "January", 'feb' = "February", 'mar' = "March", 'apr' = "April", 'may' = "May", 'jun' = "June", 'jul' = "July", 'aug' = "August", 'sep' = "September", 'oct' = "October", 'nov' = "November", 'dec' = "December")) %>%
  arrange(year, month) %>%
  select(year, month, everything())

view(unemployment)
```

## Merging datasets

The code below is used to merge the pols_month and snp datasets and then
merge the unemployment dataset with the aforementioned result.

``` r
pols_snp = 
  left_join(pols_month, snp, by = c("year", "month"))

view(pols_snp)

pols_snp_unemployment =
  left_join(pols_snp, unemployment, by = c("year", "month"))

view(pols_snp_unemployment)
```

The pols_month dataset consisted of 822 rows and 9 columns. The
variables in this dataset were year, month, gov_gop, sen_gop, rep_gop,
gov_dem, sen_dem, rep_dem, and president. The snp dataset consisted of
787 rows and 3 columns. The variables in this dataset were year, month,
and close, which details the closing prices of the S&P stock index. The
unemployment dataset consisted of 816 rows and 3 columns. The variables
in this dataset were year, month, and unemployment_percentage, which
detailed the unemployment percentages for the corresponding month and
years. The final dataset, pols_snp_unemployment, consists of all three
of the aforementioned datasets. The pols_snp_unemployment dataset
consists of 822 rows and 11 columns. This dataset consists of a range of
years that goes from 1947 to 2015. The variables in the
pols_snp_unemployment dataset are year, month, gov_gop, sen_gop,
rep_gop, gov_dem, sen_dem, rep_dem, president, close, and
unemployment_percentage. It is important to note that we are able to
merge the three aforementioned datasets together, because all three
datasets had two variables in common, which were year and month.
