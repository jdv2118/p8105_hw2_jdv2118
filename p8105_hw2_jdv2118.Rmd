---
title: "p8105_hw2_jdv2118"
output: github_document
---
# Loading libraries

The code below is used to load the libraries that are needed to complete the problems for homework 2.

```{r, message = FALSE}
library(tidyverse)

library(readxl)

library(dplyr)

library(janitor)

library(tidyr)

library(lubridate)
```

# Problem 1

Please note that the solution for Problem 1 of homework 2 for Data Science I was provided by Dr. Goldsmith prior to the due date of homework 2.  This code is seen below to use for future reference.

Below we import and clean data from `NYC_Transit_Subway_Entrance_And_Exit_Data.csv`. The process begins with data import, updates variable names, and selects the columns that will be used in later parts fo this problem. We update `entry` from `yes` / `no` to a logical variable. As part of data import, we specify that `Route` columns 8-11 should be character for consistency with 1-7.

```{r}
trans_ent = 
  read_csv(
    "data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv",
    col_types = cols(Route8 = "c", Route9 = "c", Route10 = "c", Route11 = "c")) %>% 
  janitor::clean_names() %>% 
  select(
    line, station_name, station_latitude, station_longitude, 
    starts_with("route"), entry, exit_only, vending, entrance_type, 
    ada) %>% 
  mutate(entry = ifelse(entry == "YES", TRUE, FALSE))
```

As it stands, these data are not "tidy": route number should be a variable, as should route. That is, to obtain a tidy dataset we would need to convert `route` variables from wide to long format. This will be useful when focusing on specific routes, but may not be necessary when considering questions that focus on station-level variables. 

The following code chunk selects station name and line, and then uses `distinct()` to obtain all unique combinations. As a result, the number of rows in this dataset is the number of unique stations.

```{r}
trans_ent %>% 
  select(station_name, line) %>% 
  distinct
```

The next code chunk is similar, but filters according to ADA compliance as an initial step. This produces a dataframe in which the number of rows is the number of ADA compliant stations. 

```{r}
trans_ent %>% 
  filter(ada == TRUE) %>% 
  select(station_name, line) %>% 
  distinct
```

To compute the proportion of station entrances / exits without vending allow entrance, we first exclude station entrances that do not allow vending. Then, we focus on the `entry` variable -- this logical, so taking the mean will produce the desired proportion (recall that R will coerce logical to numeric in cases like this).

```{r}
trans_ent %>% 
  filter(vending == "NO") %>% 
  pull(entry) %>% 
  mean
```

Lastly, we write a code chunk to identify stations that serve the A train, and to assess how many of these are ADA compliant. As a first step, we tidy the data as alluded to previously; that is, we convert `route` from wide to long format. After this step, we can use tools from previous parts of the question (filtering to focus on the A train, and on ADA compliance; selecting and using `distinct` to obtain dataframes with the required stations in rows).

```{r}
trans_ent %>% 
  pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") %>% 
  filter(route == "A") %>% 
  select(station_name, line) %>% 
  distinct

trans_ent %>% 
  pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") %>% 
  filter(route == "A", ada == TRUE) %>% 
  select(station_name, line) %>% 
  distinct
```

# Problem 2

## Mr. Trash Wheel

The code below is used to read and clean the "Mr. Trash Wheel" sheet.

```{r}
mr_trash_wheel = 
  read_excel("data/Trash Wheel Collection Data.xlsx", "Mr. Trash Wheel", range = "A2:N550") %>%
  clean_names(dat = .) %>%
  mutate(
  sports_balls = 
  as.integer(
  round(sports_balls)), 
  trash_wheel_id = 1, 
  year =
  as.integer(year)) %>%
  drop_na(data = ., dumpster) %>%
  select(trash_wheel_id, everything())

view(mr_trash_wheel)
```

## Professor Trash Wheel

The code below is used to import, clean, and organize the data for the "Professor Trash Wheel" sheet.

```{r}
prof_trash_wheel = 
  read_excel("data/Trash Wheel Collection Data.xlsx", "Professor Trash Wheel", range =           "A2:M97") %>%
  clean_names(dat = .) %>%
  mutate(
  trash_wheel_id = 2) %>%
  drop_na(data = ., dumpster) %>%
  select(trash_wheel_id, everything())

view(prof_trash_wheel)
```

## Combined dataset 

```{r}
trash_wheel_tidy =
  bind_rows(mr_trash_wheel, prof_trash_wheel)

view(trash_wheel_tidy)
```

The final dataset, trash_wheel_tidy, combines data from both the Mr. Trash Wheel and Professor Trash Wheel datasets.  The mr_trash_wheel dataset consists of `r nrow(mr_trash_wheel)` rows and `r ncol(mr_trash_wheel)` columns. The prof_trash_wheel dataset consists of `r nrow(prof_trash_wheel)` rows and `r ncol(prof_trash_wheel)` columns.  The trash_wheel_tidy dataset consists of `r nrow(trash_wheel_tidy)` observations.  This dataset also consists of `r ncol(trash_wheel_tidy)` variables, which are  
trash_wheel_id, dumpster, month, year, date, weight_tons, volume_cubic_yards, plastic_bottles, polystyrene, cigarette_butts, glass_bottles, grocery_bags, chip_bags, sports_balls, and homes_powered.  It is important to note that the trash_wheel_id variable is imperative as it indicates Mr. Trash Wheel for the value of 1 and indicates Professor Trash Wheel for the value of 2.  In addition, the Professor Trash Wheel dataset consisted of all the same variables as the Mr. Trash Wheel dataset except for the sports_balls variable.  This dataset shows that the total weight of trash that was collected by Professor Trash Wheel is 
`r sum(pull(prof_trash_wheel, weight_tons))` tons.  This dataset also shows that the total number of sports balls that were collected by Mr. Trash Wheel in 2020 was `r sum(pull(filter(mr_trash_wheel, year == 2020), sports_balls))`.

# Problem 3 

## Pols_month dataset

The code below is used to clean the data within the pols_month dataset.

```{r, message = FALSE}
pols_month =
  read_csv("data/pols-month.csv") %>%
  clean_names(dat = .) %>%
  separate(col = mon,   into = c("year", "month", "day"), sep = "-") %>%
  mutate(
    year = as.numeric(year), 
    month = as.numeric(month), 
    day = as.numeric(day),
    month = month.name[as.numeric(month)],
    president = 
      if_else(prez_gop == 1, 'gop', 'dem')) %>% 
    select(-prez_dem, -prez_gop, -day) %>%
  arrange(year, month)

view(pols_month)
```

## Snp dataset

The code below is used to clean the data within the snp dataset.

```{r, message = FALSE}
snp = 
  read_csv("data/snp.csv") %>%
  clean_names(dat = .) %>%
  mutate(date = parse_date_time2(date,'mdy', cutoff_2000 = 49)) %>%
  separate(col = date,   into = c("year", "month", "day"), sep = "-") %>%
  mutate( 
    year = as.numeric(year),
    month = month.name[as.numeric(month)]) %>% 
    select(-day) %>%
  arrange(year, month) %>%
  select(year, month, everything())

view(snp)
```

## Unemployment dataset

The code below is used to clean the data within the unemployment dataset.

```{r, message = FALSE}
unemployment =
  read_csv("data/unemployment.csv") %>%
  clean_names(dat = .) %>%
  pivot_longer(
    jan:dec,
    names_to = "month", 
    values_to = "unemployment_percentage"
    ) %>%
  mutate(month = recode(month, 'jan' = "January", 'feb' = "February", 'mar' = "March", 'apr' = "April", 'may' = "May", 'jun' = "June", 'jul' = "July", 'aug' = "August", 'sep' = "September", 'oct' = "October", 'nov' = "November", 'dec' = "December")) %>%
  arrange(year, month) %>%
  select(year, month, everything())

view(unemployment)
```

## Merging datasets

The code below is used to merge the pols_month and snp datasets and then merge the unemployment dataset with the aforementioned result.

```{r, message = FALSE}
pols_snp = 
  left_join(pols_month, snp, by = c("year", "month"))

view(pols_snp)

pols_snp_unemployment =
  left_join(pols_snp, unemployment, by = c("year", "month"))

view(pols_snp_unemployment)
```

The pols_month dataset consisted of `r nrow(pols_month)` rows and `r ncol(pols_month)` columns. The variables in this dataset were year, month, gov_gop, sen_gop, rep_gop, gov_dem, sen_dem, rep_dem, and president. The snp dataset consisted of `r nrow(snp)` rows and `r ncol(snp)` columns. The variables in this dataset were year, month, and close, which details the closing prices of the S&P stock index. The unemployment dataset consisted of `r nrow(unemployment)` rows and `r ncol(unemployment)` columns. The variables in this dataset were year, month, and unemployment_percentage, which detailed the unemployment percentages for the corresponding month and years.  The final dataset, pols_snp_unemployment, consists of all three of the aforementioned datasets. The pols_snp_unemployment dataset consists of `r nrow(pols_snp_unemployment)` rows and `r ncol(pols_snp_unemployment)` columns.  This dataset consists of a range of years that goes from `r min(pull(pols_snp_unemployment, year))` to `r max(pull(pols_snp_unemployment, year))`. The variables in the pols_snp_unemployment dataset are year, month, gov_gop, sen_gop, rep_gop, gov_dem, sen_dem, rep_dem, president, close, and unemployment_percentage. It is important to note that we are able to merge the three aforementioned datasets together, because all three datasets had two variables in common, which were year and month.
